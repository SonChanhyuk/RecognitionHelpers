{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caa9dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730d2ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(EmotionModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        out, _ = pad_packed_sequence(out, batch_first=True)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6442f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, csv, n_mfcc, max_len=None):\n",
    "        file_list = pd.read_csv(csv)\n",
    "        self.mfccs = []\n",
    "        self.emotions = []\n",
    "        self.max_len = 0\n",
    "        emotion_to_int = {'anger': 0, 'angry': 0, 'disgust': 1, 'fear': 2, 'happiness': 3,\n",
    "                               'neutral': 4, 'sad': 5, 'sadness': 5, 'surprise': 6}\n",
    "\n",
    "        for i in tqdm(range(len(file_list)), desc=\"Loading Data\"):\n",
    "            name = \"datasets/emotion_audio_data/{}.wav\".format(file_list.iloc[i, 1])\n",
    "            y, sr = librosa.load(name, sr=None)\n",
    "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "            if max_len:\n",
    "                mfcc = np.pad(mfcc, ((0, 0), (0, max_len - mfcc.shape[1])), 'constant', constant_values=(0))\n",
    "            self.max_len = max(self.max_len, mfcc.shape[1])\n",
    "            mfcc = torch.from_numpy(mfcc.astype(np.float32))\n",
    "            self.mfccs.append(mfcc)\n",
    "\n",
    "            emotion = file_list.iloc[i, 3]\n",
    "            self.emotions.append(emotion_to_int[emotion])\n",
    "        self.len = len(file_list)\n",
    "        self.n_mfcc = n_mfcc\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc = self.mfccs[idx]\n",
    "        label = self.emotions[idx]\n",
    "        return mfcc, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf52f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mfcc = 13\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f362a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    mfccs = [item[0].T for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    \n",
    "    # Padding\n",
    "    mfccs = pad_sequence(mfccs, batch_first=True)\n",
    "    \n",
    "    return mfccs, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42505a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 35179/35179 [11:56<00:00, 49.13it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = EmotionDataset(\"datasets/emotion_train.csv\", n_mfcc)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f33750d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8793/8793 [03:00<00:00, 48.59it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = EmotionDataset(\"datasets/emotion_test.csv\", n_mfcc)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "302a28a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          collate_fn=collate_fn,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bb06356",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                         collate_fn=collate_fn,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79c47b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ca0bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mfcc = 13\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "num_classes = 7\n",
    "learning_rate = 0.01\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54a18201",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmotionModel(n_mfcc, hidden_size, num_layers, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e5f5ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] Loss: 2.1395:  86%|███████████████████████████████████████████▊       | 1889/2199 [04:29<00:44,  7.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Update progress bar description\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Initialize tqdm with total number of batches in an epoch\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, (mfccs, labels) in progress_bar:\n",
    "        mfccs = mfccs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(mfccs, [mfccs.shape[1]] * len(mfccs))\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar description\n",
    "        progress_bar.set_description(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc617df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmotionModel(\n",
       "  (lstm): LSTM(13, 512, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb25995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de85546a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a67585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e401842a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f5f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7902b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, csv_file, audio_dir, max_pad_len):\n",
    "        self.emotion_frame = pd.read_csv(csv_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.max_pad_len = max_pad_len\n",
    "        self.emotion_to_int = {'anger': 0, 'angry': 0, 'disgust': 1, 'fear': 2, 'happiness': 3,\n",
    "                               'neutral': 4, 'sad': 5, 'sadness': 5, 'surprise': 6}\n",
    "        self.mfccs = []\n",
    "        self.emotions = []\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        for idx in tqdm(range(len(self.emotion_frame))):\n",
    "            file_path = os.path.join(self.audio_dir, self.emotion_frame.iloc[idx, 1]) + \".wav\"\n",
    "            y, sr = librosa.load(file_path, duration=3)  # 음성 파일 로드 (3초로 제한)\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, n_fft=2048, hop_length=512)\n",
    "            pad_width = self.max_pad_len - mfccs.shape[1]\n",
    "            mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "            mfccs = mfccs.transpose()\n",
    "            emotion = self.emotion_frame.iloc[idx, 3]\n",
    "            emotion = self.emotion_to_int[emotion]\n",
    "\n",
    "            self.mfccs.append(torch.tensor(mfccs, dtype=torch.float32))\n",
    "            self.emotions.append(torch.tensor(emotion, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emotion_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfccs = self.mfccs[idx]\n",
    "        emotion = self.emotions[idx]\n",
    "\n",
    "        return {'mfccs': mfccs, 'emotion': emotion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a4f4d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, csv_file, audio_dir, max_pad_len):\n",
    "        self.emotion_frame = pd.read_csv(csv_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.max_pad_len = max_pad_len\n",
    "        self.emotion_to_int = {'anger': 0, 'angry': 0, 'disgust': 1, 'fear': 2, 'happiness': 3, \n",
    "                           'neutral': 4, 'sad': 5, 'sadness': 5, 'surprise': 6}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emotion_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.audio_dir, self.emotion_frame.iloc[idx, 1])+\".wav\"\n",
    "        y, sr = librosa.load(file_path, duration=3)  # 음성 파일 로드 (3초로 제한)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, n_fft=2048, hop_length=512)\n",
    "        pad_width = self.max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        mfccs = mfccs.transpose()\n",
    "        emotion = self.emotion_frame.iloc[idx, 3]\n",
    "        emotion = self.emotion_to_int[emotion]\n",
    "        \n",
    "        mfccs = torch.tensor(mfccs, dtype=torch.float32)\n",
    "        emotion = torch.tensor(emotion, dtype=torch.long)\n",
    "\n",
    "        return {'mfccs': mfccs, 'emotion': emotion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f2016e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 클래스 정의\n",
    "class EmotionRecognitionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(EmotionRecognitionModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5538fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "batch_size = 32\n",
    "input_size = 13  # MFCC 특징의 크기\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 9\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "972f7217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 35179/35179 [48:41<00:00, 12.04it/s]\n",
      "Epoch [1/10], Loss: 1.7434: 100%|██████████████████████████████████████████████████| 1100/1100 [00:20<00:00, 53.01it/s]\n",
      "Epoch [2/10], Loss: 1.7358: 100%|██████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 62.49it/s]\n",
      "Epoch [3/10], Loss: 1.7361: 100%|██████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 62.36it/s]\n",
      "Epoch [4/10], Loss: 1.7344: 100%|██████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 62.34it/s]\n",
      "Epoch [5/10], Loss: 1.7342: 100%|██████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 62.41it/s]\n",
      "Epoch [6/10], Loss: 1.7339: 100%|██████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 62.34it/s]\n",
      "Epoch [7/10], Loss: 1.7342: 100%|██████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 62.37it/s]\n",
      "Epoch [8/10], Loss: 1.7338: 100%|██████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 62.43it/s]\n",
      "Epoch [9/10], Loss: 1.7339: 100%|██████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 62.18it/s]\n",
      "Epoch [10/10], Loss: 1.7336: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 62.23it/s]\n",
      "Epoch [11/10], Loss: 1.7332: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 62.21it/s]\n",
      "Epoch [12/10], Loss: 1.7335: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 62.14it/s]\n",
      "Epoch [13/10], Loss: 1.7334: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.77it/s]\n",
      "Epoch [14/10], Loss: 1.7335: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.69it/s]\n",
      "Epoch [15/10], Loss: 1.7333: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.68it/s]\n",
      "Epoch [16/10], Loss: 1.7334: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.65it/s]\n",
      "Epoch [17/10], Loss: 1.7331: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.64it/s]\n",
      "Epoch [18/10], Loss: 1.7331: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.61it/s]\n",
      "Epoch [19/10], Loss: 1.7332: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.63it/s]\n",
      "Epoch [20/10], Loss: 1.7330: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.36it/s]\n",
      "Epoch [21/10], Loss: 1.7330: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.70it/s]\n",
      "Epoch [22/10], Loss: 1.7329: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.75it/s]\n",
      "Epoch [23/10], Loss: 1.7329: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.70it/s]\n",
      "Epoch [24/10], Loss: 1.7331: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.67it/s]\n",
      "Epoch [25/10], Loss: 1.7332: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.69it/s]\n",
      "Epoch [26/10], Loss: 1.7331: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.62it/s]\n",
      "Epoch [27/10], Loss: 1.7331: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.56it/s]\n",
      "Epoch [28/10], Loss: 1.7332: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.66it/s]\n",
      "Epoch [29/10], Loss: 1.7330: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.65it/s]\n",
      "Epoch [30/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.61it/s]\n",
      "Epoch [31/10], Loss: 1.7333: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.61it/s]\n",
      "Epoch [32/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.67it/s]\n",
      "Epoch [33/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.70it/s]\n",
      "Epoch [34/10], Loss: 1.7329: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.69it/s]\n",
      "Epoch [35/10], Loss: 1.7328: 100%|█████████████████████████████████████████████████| 1100/1100 [00:18<00:00, 61.10it/s]\n",
      "Epoch [36/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.68it/s]\n",
      "Epoch [37/10], Loss: 1.7328: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.51it/s]\n",
      "Epoch [38/10], Loss: 1.7328: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.57it/s]\n",
      "Epoch [39/10], Loss: 1.7330: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.61it/s]\n",
      "Epoch [40/10], Loss: 1.7328: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.60it/s]\n",
      "Epoch [41/10], Loss: 1.7330: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.60it/s]\n",
      "Epoch [42/10], Loss: 1.7332: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.58it/s]\n",
      "Epoch [43/10], Loss: 1.7329: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.65it/s]\n",
      "Epoch [44/10], Loss: 1.7330: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.69it/s]\n",
      "Epoch [45/10], Loss: 1.7329: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.65it/s]\n",
      "Epoch [46/10], Loss: 1.7330: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.62it/s]\n",
      "Epoch [47/10], Loss: 1.7329: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.63it/s]\n",
      "Epoch [48/10], Loss: 1.7329: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.61it/s]\n",
      "Epoch [49/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.69it/s]\n",
      "Epoch [50/10], Loss: 1.7330: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.57it/s]\n",
      "Epoch [51/10], Loss: 1.7328: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.56it/s]\n",
      "Epoch [52/10], Loss: 1.7329: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.52it/s]\n",
      "Epoch [53/10], Loss: 1.7328: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.59it/s]\n",
      "Epoch [54/10], Loss: 1.7329: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.53it/s]\n",
      "Epoch [55/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.58it/s]\n",
      "Epoch [56/10], Loss: 1.7328: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.60it/s]\n",
      "Epoch [57/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.61it/s]\n",
      "Epoch [58/10], Loss: 1.7328: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.67it/s]\n",
      "Epoch [59/10], Loss: 1.7326: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.67it/s]\n",
      "Epoch [60/10], Loss: 1.7329: 100%|█████████████████████████████████████████████████| 1100/1100 [00:18<00:00, 60.08it/s]\n",
      "Epoch [61/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:18<00:00, 58.58it/s]\n",
      "Epoch [62/10], Loss: 1.7325: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.53it/s]\n",
      "Epoch [63/10], Loss: 1.7328: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.44it/s]\n",
      "Epoch [64/10], Loss: 1.7330: 100%|█████████████████████████████████████████████████| 1100/1100 [00:18<00:00, 61.00it/s]\n",
      "Epoch [65/10], Loss: 1.7328: 100%|█████████████████████████████████████████████████| 1100/1100 [00:18<00:00, 60.68it/s]\n",
      "Epoch [66/10], Loss: 1.7329: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.43it/s]\n",
      "Epoch [67/10], Loss: 1.7326: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.55it/s]\n",
      "Epoch [68/10], Loss: 1.7328: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.82it/s]\n",
      "Epoch [69/10], Loss: 1.7326: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.78it/s]\n",
      "Epoch [70/10], Loss: 1.7325: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.76it/s]\n",
      "Epoch [71/10], Loss: 1.7329: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.75it/s]\n",
      "Epoch [72/10], Loss: 1.7326: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.77it/s]\n",
      "Epoch [73/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.81it/s]\n",
      "Epoch [74/10], Loss: 1.7329: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.80it/s]\n",
      "Epoch [75/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.67it/s]\n",
      "Epoch [76/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.66it/s]\n",
      "Epoch [77/10], Loss: 1.7328: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.62it/s]\n",
      "Epoch [78/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.83it/s]\n",
      "Epoch [79/10], Loss: 1.7329: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.86it/s]\n",
      "Epoch [80/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.78it/s]\n",
      "Epoch [81/10], Loss: 1.7324: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.71it/s]\n",
      "Epoch [82/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.78it/s]\n",
      "Epoch [83/10], Loss: 1.7325: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.75it/s]\n",
      "Epoch [84/10], Loss: 1.7326: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.64it/s]\n",
      "Epoch [85/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.67it/s]\n",
      "Epoch [86/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.64it/s]\n",
      "Epoch [87/10], Loss: 1.7326: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.72it/s]\n",
      "Epoch [88/10], Loss: 1.7326: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.68it/s]\n",
      "Epoch [89/10], Loss: 1.7329: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.69it/s]\n",
      "Epoch [90/10], Loss: 1.7328: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.78it/s]\n",
      "Epoch [91/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.68it/s]\n",
      "Epoch [92/10], Loss: 1.7330: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.80it/s]\n",
      "Epoch [93/10], Loss: 1.7326: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.81it/s]\n",
      "Epoch [94/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.65it/s]\n",
      "Epoch [95/10], Loss: 1.7326: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.72it/s]\n",
      "Epoch [96/10], Loss: 1.7326: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.66it/s]\n",
      "Epoch [97/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.63it/s]\n",
      "Epoch [98/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.68it/s]\n",
      "Epoch [99/10], Loss: 1.7327: 100%|█████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.67it/s]\n",
      "Epoch [100/10], Loss: 1.7328: 100%|████████████████████████████████████████████████| 1100/1100 [00:17<00:00, 61.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋과 데이터 로더 생성\n",
    "train_csv_file = 'datasets/emotion_train.csv'\n",
    "audio_dir = 'datasets/emotion_audio_data/'\n",
    "max_pad_len = 1000  # 패딩 길이\n",
    "train_dataset = EmotionDataset(train_csv_file, audio_dir, max_pad_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 모델 초기화\n",
    "model = EmotionRecognitionModel(input_size, hidden_size, num_layers, num_classes)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(num_epochs*10):\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader)\n",
    "    model.train()\n",
    "    for i, sample in enumerate(progress_bar):\n",
    "        inputs, labels = sample['mfccs'].to(device), sample['emotion'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_description(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / (i+1):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf87e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9219bb13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058c26d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0562b696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "677196ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import librosa\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None, max_pad_len=1057):\n",
    "        self.emotion_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.emotion_to_int = {'anger': 0, 'angry': 0, 'disgust': 1, 'fear': 2, 'happiness': 3, \n",
    "                           'neutral': 4, 'sad': 5, 'sadness': 5, 'surprise': 6}\n",
    "        self.max_pad_len = max_pad_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.emotion_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        wav_name = os.path.join(self.root_dir,\n",
    "                                self.emotion_frame.iloc[idx, 1])+\".wav\"\n",
    "        y, sr = librosa.load(wav_name, res_type='kaiser_fast')\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "        pad_width = max(self.max_pad_len - mfccs.shape[1], 0)\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        mfccs = torch.tensor(mfccs)\n",
    "\n",
    "\n",
    "        mfccs = mfccs.transpose()  # Transpose the matrix to align the input size\n",
    "\n",
    "        emotion = self.emotion_frame.iloc[idx, 3]\n",
    "        emotion = self.emotion_to_int[emotion]\n",
    "        emotion = torch.tensor(emotion)\n",
    "\n",
    "        sample = {'mfccs': mfccs, 'emotion': emotion}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    mfccs = [item['mfccs'] for item in batch]\n",
    "    emotions = [item['emotion'] for item in batch]\n",
    "    mfccs_padded = pad_sequence(mfccs, batch_first=True)\n",
    "    emotions = torch.stack(emotions)\n",
    "    return {'mfccs': mfccs_padded, 'emotion': emotions}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3813bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(EmotionModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(x.device)\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(x.device)\n",
    "\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5354e311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/2199 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "transpose() received an invalid combination of arguments - got (), but expected one of:\n * (int dim0, int dim1)\n * (name dim0, name dim1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     17\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader)\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(progress_bar):\n\u001b[0;32m     19\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmfccs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mEmotionDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m mfccs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpad(mfccs, pad_width\u001b[38;5;241m=\u001b[39m((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m0\u001b[39m, pad_width)), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m mfccs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(mfccs)\n\u001b[1;32m---> 33\u001b[0m mfccs \u001b[38;5;241m=\u001b[39m \u001b[43mmfccs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Transpose the matrix to align the input size\u001b[39;00m\n\u001b[0;32m     35\u001b[0m emotion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memotion_frame\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m     36\u001b[0m emotion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memotion_to_int[emotion]\n",
      "\u001b[1;31mTypeError\u001b[0m: transpose() received an invalid combination of arguments - got (), but expected one of:\n * (int dim0, int dim1)\n * (name dim0, name dim1)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = EmotionModel(input_dim=40, hidden_dim=256, layer_dim=2, output_dim=7).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "emotion_dataset = EmotionDataset(csv_file='datasets/emotion_train.csv', root_dir='datasets/emotion_audio_data/')\n",
    "train_loader = DataLoader(emotion_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader)\n",
    "    for i, sample in enumerate(progress_bar):\n",
    "        inputs, labels = sample['mfccs'].to(device), sample['emotion'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_description(f'Loss: {loss.item()}')\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss: {epoch_loss/len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4a09147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = {}\n",
    "label_index[\"anger\"] = 0\n",
    "label_index[\"angry\"] = 0\n",
    "label_index[\"disgust\"] = 1\n",
    "label_index[\"fear\"] = 2\n",
    "label_index[\"happiness\"] = 3\n",
    "label_index[\"neutral\"] = 4\n",
    "label_index[\"sad\"] = 5 \n",
    "label_index[\"sadness\"] = 5\n",
    "label_index[\"surprise\"] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "47657578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "import os\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_file = os.path.join(\"datasets/emotion_audio_data/\", self.df.iloc[index, 1]+\".wav\")\n",
    "        label = torch.tensor(label_index[self.df.iloc[index, 3]])\n",
    "        audio =  torch.from_numpy(preprocess_audio(audio_file))\n",
    "        return audio, label, audio.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "96c5c2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    data.sort(key=lambda x: x[2], reverse=True)\n",
    "    audios, labels, lengths = zip(*data)\n",
    "    audios = pad_sequence(audios, batch_first=True)\n",
    "    labels = torch.tensor(labels)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    return audios, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "71ade813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create the dataset\n",
    "dataset = AudioDataset(csv_file='datasets/emotion_train.csv')\n",
    "loader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2546c7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input must have 2 dimensions, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [73]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36mEmotionRecognitionModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39mbatch_sizes[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[0;32m     16\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39mbatch_sizes[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 18\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# out: tensor of shape (batch_size, seq_length, hidden_size)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:767\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 767\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    769\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    770\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:692\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    688\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    689\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    690\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    691\u001b[0m                        ):\n\u001b[1;32m--> 692\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    694\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    696\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:201\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    199\u001b[0m expected_input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m expected_input_dim:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    203\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    207\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input must have 2 dimensions, got 1"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create the model\n",
    "model = EmotionRecognitionModel(input_size=40, hidden_size=50, num_layers=2, num_classes=5).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels, lengths) in tqdm(enumerate(loader)):\n",
    "        inputs = pack_padded_sequence(inputs, lengths, batch_first=True, enforce_sorted=False)\n",
    "        labels = labels.to(device, dtype=torch.long)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(loader)}\")\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4442ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/emotion_audio_data/5f04edc9b140144dfcfef53e.wav\n",
      "tensor(0)\n",
      "[[-80.       -80.       -80.       ... -42.019623 -40.404594 -42.039497]\n",
      " [-80.       -80.       -80.       ... -55.069542 -56.325912 -55.70308 ]\n",
      " [-80.       -80.       -80.       ... -57.148438 -58.260925 -62.47307 ]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5f6eab4f111dfd48d40fd27c.wav\n",
      "tensor(5)\n",
      "[[-80.       -80.       -80.       ... -25.060246 -24.680363 -24.126091]\n",
      " [-80.       -80.       -80.       ... -28.653961 -29.755291 -31.59039 ]\n",
      " [-80.       -80.       -80.       ... -33.605335 -34.63547  -36.764503]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -67.442665 -67.25614  -65.94096 ]\n",
      " [-80.       -80.       -80.       ... -74.91232  -74.276596 -74.04675 ]\n",
      " [-80.       -80.       -80.       ... -74.62927  -74.446205 -74.204765]]\n",
      "datasets/emotion_audio_data/5e37f1f233e9ad176cc9b415.wav\n",
      "tensor(5)\n",
      "[[-80.       -56.26912  -36.760155 ... -32.572334 -30.04596  -28.601265]\n",
      " [-80.       -62.764893 -37.93434  ... -36.938892 -37.907272 -43.94189 ]\n",
      " [-80.       -60.820084 -41.303726 ... -37.291092 -38.102562 -41.53329 ]\n",
      " ...\n",
      " [-80.       -80.       -79.31967  ... -70.22738  -70.15658  -68.684   ]\n",
      " [-80.       -80.       -78.8933   ... -76.83051  -76.07894  -75.31406 ]\n",
      " [-80.       -80.       -79.06405  ... -77.21224  -76.56014  -76.1531  ]]\n",
      "datasets/emotion_audio_data/5e40eee9189842034d9f7137.wav\n",
      "tensor(0)\n",
      "[[-80.       -68.53071  -49.78485  ... -46.873104 -51.011494 -54.028362]\n",
      " [-80.       -72.59299  -54.23659  ... -64.996826 -64.70393  -62.11339 ]\n",
      " [-80.       -80.       -68.1611   ... -67.77437  -68.08386  -68.02724 ]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5fb9fd124c55eb78bd7ce123.wav\n",
      "tensor(5)\n",
      "[[-80.       -80.       -80.       ... -46.160477 -38.722923 -34.199135]\n",
      " [-80.       -80.       -80.       ... -47.983986 -45.839306 -45.33691 ]\n",
      " [-80.       -80.       -80.       ... -51.345684 -50.00478  -51.030476]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -69.41059  -69.83415  -69.78541 ]\n",
      " [-80.       -80.       -80.       ... -72.66052  -72.50643  -72.41208 ]\n",
      " [-80.       -80.       -80.       ... -72.54323  -72.69039  -73.04809 ]]\n",
      "datasets/emotion_audio_data/5efcbdfb704f492ee1253902.wav\n",
      "tensor(0)\n",
      "[[-80.       -80.       -80.       ... -63.137924 -63.708687 -61.635002]\n",
      " [-80.       -80.       -80.       ... -65.81025  -67.00804  -65.23995 ]\n",
      " [-80.       -80.       -80.       ... -68.84057  -67.59937  -68.370865]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5f09a95fb140144dfcff1c53.wav\n",
      "tensor(5)\n",
      "[[-80.       -80.       -80.       ... -43.91799  -43.620407 -43.912506]\n",
      " [-80.       -80.       -80.       ... -49.745018 -49.73091  -51.87316 ]\n",
      " [-80.       -80.       -80.       ... -60.62211  -61.398445 -62.08937 ]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5fba3b2c44697678c497b417.wav\n",
      "tensor(0)\n",
      "[[-80.       -80.       -70.23933  ... -59.242146 -58.20837  -58.94235 ]\n",
      " [-80.       -80.       -74.1465   ... -58.41088  -57.59852  -57.26069 ]\n",
      " [-80.       -80.       -79.075005 ... -60.023422 -59.667747 -62.71212 ]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5e47ccf27bef803b4851d901.wav\n",
      "tensor(2)\n",
      "[[-80.       -74.86479  -58.97664  ... -35.093555 -36.294464 -36.753628]\n",
      " [-80.       -69.905266 -55.76243  ... -42.846172 -42.388596 -40.470974]\n",
      " [-80.       -69.36837  -56.20786  ... -44.561928 -44.665253 -44.58291 ]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -77.658295 -77.27609  -75.10254 ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5e338a4f5807b852d9e04dfe.wav\n",
      "tensor(5)\n",
      "[[-80.       -59.908386 -46.30829  ... -37.97496  -37.64814  -35.226097]\n",
      " [-80.       -58.152718 -45.203392 ... -49.581886 -47.145615 -45.921867]\n",
      " [-80.       -54.327675 -41.557495 ... -54.78695  -58.32093  -56.76583 ]\n",
      " ...\n",
      " [-80.       -80.       -79.6338   ... -72.527756 -72.71089  -73.54963 ]\n",
      " [-80.       -80.       -80.       ... -77.458496 -76.94266  -77.28231 ]\n",
      " [-80.       -80.       -80.       ... -76.4226   -77.36754  -78.33666 ]]\n",
      "datasets/emotion_audio_data/5e36c4f6c8c25f16cd145023.wav\n",
      "tensor(0)\n",
      "[[-80.       -80.       -80.       ... -39.785797 -45.94326  -44.623894]\n",
      " [-80.       -80.       -80.       ... -43.921654 -42.096817 -39.256065]\n",
      " [-80.       -80.       -80.       ... -43.79886  -41.078575 -40.584732]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -75.52184  -74.946815 -75.067986]\n",
      " [-80.       -80.       -80.       ... -79.24907  -79.90601  -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5fbc82764c55eb78bd7ce863.wav\n",
      "tensor(0)\n",
      "[[-80.       -80.       -80.       ... -58.574543 -57.426994 -60.028297]\n",
      " [-80.       -80.       -80.       ... -69.112    -71.114456 -68.1431  ]\n",
      " [-80.       -80.       -80.       ... -76.31497  -73.75256  -72.201935]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5f5b621eb87813162834981c.wav\n",
      "tensor(4)\n",
      "[[-80.       -80.       -80.       ... -29.034067 -26.051357 -24.78289 ]\n",
      " [-80.       -80.       -80.       ... -30.870935 -33.704338 -35.080452]\n",
      " [-80.       -80.       -80.       ... -32.882668 -34.405952 -37.297913]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5f62dbe854b2361621284ce4.wav\n",
      "tensor(0)\n",
      "[[-80.       -80.       -80.       ... -19.91576  -12.12953  -12.571265]\n",
      " [-80.       -80.       -80.       ... -23.642212 -16.466297 -17.601877]\n",
      " [-80.       -80.       -80.       ... -34.74913  -29.314388 -30.8904  ]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -73.39698  -71.908485 -72.90223 ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5f696788f8fac448cc0a5c23.wav\n",
      "tensor(6)\n",
      "[[-80.       -80.       -73.6559   ... -40.866127 -46.48137  -47.86932 ]\n",
      " [-80.       -80.       -76.70583  ... -59.981106 -58.341892 -58.164196]\n",
      " [-80.       -80.       -76.436226 ... -66.12955  -64.59115  -63.79241 ]\n",
      " ...\n",
      " [-80.       -80.       -79.74108  ... -79.805084 -79.19443  -78.0747  ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5ece4d969aa8ea0eec53ef24.wav\n",
      "tensor(2)\n",
      "[[-80.       -80.       -80.       ... -55.200706 -56.05063  -58.000366]\n",
      " [-80.       -80.       -80.       ... -57.33915  -57.499992 -56.635857]\n",
      " [-80.       -80.       -80.       ... -52.27539  -56.672043 -59.23813 ]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5f032c69b140144dfcfedeea.wav\n",
      "tensor(0)\n",
      "[[-80.       -80.       -80.       ... -36.95407  -43.035572 -50.69791 ]\n",
      " [-80.       -80.       -80.       ... -46.882545 -45.522133 -48.326286]\n",
      " [-80.       -80.       -80.       ... -50.6975   -50.119514 -48.906956]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -75.871796 -73.065796 -71.94825 ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5ec9b5641dcf350eeded464f.wav\n",
      "tensor(5)\n",
      "[[-80.       -80.       -80.       ... -58.62687  -57.794968 -59.198402]\n",
      " [-80.       -80.       -80.       ... -64.61773  -64.233406 -65.41755 ]\n",
      " [-80.       -80.       -80.       ... -71.78744  -75.25109  -71.72583 ]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -76.21718  -76.769356 -77.169846]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5f66009f111dfd48d40fcac7.wav\n",
      "tensor(3)\n",
      "[[-80.       -80.       -80.       ... -37.32747  -39.877293 -40.597496]\n",
      " [-80.       -80.       -80.       ... -39.137947 -44.263252 -48.99385 ]\n",
      " [-80.       -80.       -80.       ... -43.11103  -46.12746  -46.50104 ]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5ed49dd41dcf350eeded4c2d.wav\n",
      "tensor(5)\n",
      "[[-80.       -80.       -80.       ... -32.746376 -32.111458 -32.699196]\n",
      " [-80.       -80.       -80.       ... -35.678284 -36.74417  -36.974792]\n",
      " [-80.       -80.       -80.       ... -38.493557 -38.313393 -39.53534 ]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/emotion_audio_data/5efc92f1704f492ee12537b5.wav\n",
      "tensor(5)\n",
      "[[-80.       -80.       -80.       ... -61.573643 -56.607048 -54.22828 ]\n",
      " [-80.       -80.       -80.       ... -56.21641  -52.40764  -49.5945  ]\n",
      " [-80.       -80.       -80.       ... -56.81953  -54.32172  -52.159794]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -70.63549  -69.65435  -69.574005]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5f866c93111dfd48d40fe07a.wav\n",
      "tensor(3)\n",
      "[[-80.       -80.       -77.07026  ... -54.536133 -56.085934 -54.92125 ]\n",
      " [-80.       -80.       -80.       ... -68.35004  -70.35807  -73.32697 ]\n",
      " [-80.       -80.       -80.       ... -65.473595 -66.2165   -73.487526]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -78.83131  -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5e355e625807b852d9e082d1.wav\n",
      "tensor(5)\n",
      "[[-80.       -80.       -80.       ... -47.444237 -45.752193 -46.99195 ]\n",
      " [-80.       -80.       -80.       ... -51.669693 -56.677673 -60.32274 ]\n",
      " [-80.       -80.       -80.       ... -57.37145  -61.215256 -64.83446 ]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5fb4b75fcb503578af9ed919.wav\n",
      "tensor(5)\n",
      "[[-80.       -80.       -80.       ... -12.903417 -12.830452 -12.930216]\n",
      " [-80.       -80.       -80.       ... -43.816498 -43.924076 -42.35262 ]\n",
      " [-80.       -80.       -80.       ... -44.57614  -44.66676  -43.145004]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -63.28834  -63.06533  -62.071552]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5e391b6133e9ad176cc9ba0f.wav\n",
      "tensor(5)\n",
      "[[-80.       -80.       -80.       ... -43.6665   -43.198425 -50.080383]\n",
      " [-80.       -80.       -80.       ... -46.924786 -47.591892 -55.593002]\n",
      " [-80.       -80.       -80.       ... -51.96093  -53.687057 -57.448692]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -77.75259  -78.44483  -79.48301 ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5fbca181cb503578af9ee822.wav\n",
      "tensor(3)\n",
      "[[-80. -80. -80. ... -80. -80. -80.]\n",
      " [-80. -80. -80. ... -80. -80. -80.]\n",
      " [-80. -80. -80. ... -80. -80. -80.]\n",
      " ...\n",
      " [-80. -80. -80. ... -80. -80. -80.]\n",
      " [-80. -80. -80. ... -80. -80. -80.]\n",
      " [-80. -80. -80. ... -80. -80. -80.]]\n",
      "datasets/emotion_audio_data/5f09aaaab140144dfcff1c66.wav\n",
      "tensor(0)\n",
      "[[-80.       -80.       -80.       ... -52.851334 -56.733116 -55.357353]\n",
      " [-80.       -80.       -80.       ... -62.073997 -61.641327 -60.06009 ]\n",
      " [-80.       -80.       -80.       ... -65.4524   -65.27969  -68.98552 ]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5ebf684e7bef803b4851e09a.wav\n",
      "tensor(1)\n",
      "[[-80.      -80.      -80.      ... -68.99697 -67.73278 -66.73354]\n",
      " [-80.      -80.      -80.      ... -75.26069 -74.91908 -74.86603]\n",
      " [-80.      -80.      -80.      ... -75.91438 -78.34798 -80.     ]\n",
      " ...\n",
      " [-80.      -80.      -80.      ... -80.      -80.      -80.     ]\n",
      " [-80.      -80.      -80.      ... -80.      -80.      -80.     ]\n",
      " [-80.      -80.      -80.      ... -80.      -80.      -80.     ]]\n",
      "datasets/emotion_audio_data/5f81d8fa111dfd48d40fdabd.wav\n",
      "tensor(2)\n",
      "[[-80.       -80.       -80.       ... -70.55656  -67.528984 -64.17352 ]\n",
      " [-80.       -80.       -80.       ... -65.58397  -67.1636   -65.53668 ]\n",
      " [-80.       -80.       -80.       ... -60.722404 -62.26722  -64.73295 ]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5e365c1833e9ad176cc9a6aa.wav\n",
      "tensor(1)\n",
      "[[-80.       -68.489044 -49.698868 ... -38.297325 -38.44311  -39.681793]\n",
      " [-80.       -68.266525 -51.773746 ... -51.447502 -51.320538 -47.689144]\n",
      " [-80.       -70.52351  -48.62754  ... -49.032433 -49.91441  -52.810993]\n",
      " ...\n",
      " [-68.03917  -62.48711  -56.294228 ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5fb9e3f944697678c497b1be.wav\n",
      "tensor(1)\n",
      "[[-80.       -80.       -80.       ... -43.90902  -41.132328 -38.64796 ]\n",
      " [-80.       -80.       -80.       ... -48.111397 -46.917366 -45.265785]\n",
      " [-80.       -80.       -80.       ... -52.306213 -52.453407 -50.81392 ]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "datasets/emotion_audio_data/5e367f80dbc4b7182a6a9335.wav\n",
      "tensor(2)\n",
      "[[-80.       -80.       -80.       ... -36.06581  -35.780357 -35.87719 ]\n",
      " [-80.       -80.       -80.       ... -42.647144 -41.507164 -43.204643]\n",
      " [-80.       -80.       -80.       ... -51.314957 -51.723396 -52.20253 ]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      2\u001b[0m data\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m2\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m audios, labels, lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[1;32m----> 4\u001b[0m audios \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudios\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(labels)\n\u001b[0;32m      6\u001b[0m lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(lengths)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\utils\\rnn.py:396\u001b[0m, in \u001b[0;36mpad_sequence\u001b[1;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[0;32m    392\u001b[0m         sequences \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "for i in loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b47cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c232c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
