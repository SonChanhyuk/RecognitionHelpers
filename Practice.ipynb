{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93168bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8756804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio files\n",
    "audio_dir = \"dataset\\자유대화 음성(일반남녀)\\Training\\일반남여_일반통합01_F_0.0baesubin_26_수도권_실내\"\n",
    "audio_files = os.listdir(audio_dir)\n",
    "audio_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e55b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, sr = librosa.load(os.path.join(audio_dir, audio_files[0]), sr=16000)\n",
    "signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3397be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1797784",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emphasized = librosa.effects.preemphasis(signal)\n",
    "emphasized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a311ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(emphasized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db72da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# windowing\n",
    "w = np.hanning(emphasized.size)\n",
    "windowed = w * emphasized\n",
    "windowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56617d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(windowed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fedc11",
   "metadata": {},
   "source": [
    "### windowed FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs = 16000\n",
    "T = 1/Fs\n",
    "\n",
    "Y = np.fft.fft(windowed)\n",
    "amp = abs(Y) * (2/len(Y))\n",
    "freq = np.fft.fftfreq(len(Y), T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfacf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(freq, amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484b2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlim(0, 8100)\n",
    "plt.plot(freq, amp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a8fcd4",
   "metadata": {},
   "source": [
    "### normal FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs = 16000\n",
    "T = 1/Fs\n",
    "\n",
    "Y = np.fft.fft(emphasized)\n",
    "amp = abs(Y) * (2/len(Y))\n",
    "freq = np.fft.fftfreq(len(Y), T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33410efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(freq, amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e361a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.xlim(0, 8100)\n",
    "plt.plot(freq, amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096bfd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio files\n",
    "audio_dir = \"dataset\\자유대화 음성(일반남녀)\\Training\\일반남여_일반통합01_F_0.0baesubin_26_수도권_실내\"\n",
    "audio_files = os.listdir(audio_dir)\n",
    "\n",
    "# Apply pre-emphasis and windowing to each audio file\n",
    "for file in audio_files:\n",
    "    # Load audio file\n",
    "    signal, sr = librosa.load(os.path.join(audio_dir, file), sr=16000)\n",
    "    \n",
    "    # Apply pre-emphasis\n",
    "    emphasized_signal = pre_emphasis(signal, pre_emphasis_coeff)\n",
    "    \n",
    "    # Apply windowing\n",
    "    windowed_signal = apply_window(emphasized_signal, window_size)\n",
    "    \n",
    "    # Save windowed audio file\n",
    "    librosa.output.write_wav(os.path.join(audio_dir, \"windowed_\" + file), windowed_signal, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaad42d",
   "metadata": {},
   "source": [
    "### 여기서 부터 찐임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e84a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_base_dataset():\n",
    "    # Load audio files\n",
    "    base_dir = \"dataset\\자유대화 음성(일반남녀)\\Training\"\n",
    "    folders = os.listdir(base_dir)\n",
    "\n",
    "    dataset = []\n",
    "    for folder in folders:\n",
    "        if folder == \"label\":\n",
    "            continue\n",
    "        sub_dir = os.path.join(base_dir, folder)\n",
    "        if not os.path.isdir(sub_dir):\n",
    "            continue\n",
    "\n",
    "        audio_dir = sub_dir\n",
    "        audio_files = os.listdir(audio_dir)    \n",
    "\n",
    "        label_dir =  os.path.join(base_dir, \"label\", folder)\n",
    "        label_files = os.listdir(label_dir)\n",
    "\n",
    "        # Apply pre-emphasis and windowing to each audio file\n",
    "        for file, label in zip(audio_files, label_files):\n",
    "            # Load audio file\n",
    "            signal, sr = librosa.load(os.path.join(audio_dir, file), sr=16000)\n",
    "\n",
    "            # Apply pre-emphasis\n",
    "            emphasized = librosa.effects.preemphasis(signal)\n",
    "\n",
    "            # Set window\n",
    "            w = np.hanning(emphasized.size)\n",
    "\n",
    "            # Apply windowing\n",
    "            windowed = w * emphasized\n",
    "\n",
    "            # Get the label\n",
    "            with open(os.path.join(label_dir, label), \"r\", encoding=\"UTF-8\") as js:\n",
    "                l = json.load(js)\n",
    "                sentence = l['발화정보']['stt']\n",
    "                sentence = re.sub(\"[(NO:)]|[(SP:)]\", \"\", sentence) # 마킹 태그 제거\n",
    "                sentence = re.sub(\"[.]|[,]\", \"\", sentence) # 마침표, 쉼표 제거\n",
    "                dataset.append([windowed, sentence])\n",
    "\n",
    "    df = pd.DataFrame(dataset, columns=['data','label'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff21362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 길이가 제각기 다르므로 가장 긴 길이의 데이터를 기준으로 패딩을 채운다 (RNN의 경우 이 과정이 필요없다)\n",
    "maximum = [len(row) for row in df['data']] / len()\n",
    "for i, row in enumerate(df['data'].copy(deep=True)):\n",
    "    row = np.pad(row, (0, maximum-len(row)), 'constant')\n",
    "    df['data'][i] = row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c83135",
   "metadata": {},
   "source": [
    "### MFCC 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "739a39d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(df):\n",
    "    mfccs = []\n",
    "    for j, i in enumerate(df['data']):\n",
    "        extracted_features = librosa.feature.mfcc(y=i,sr=16000,n_mfcc=40)\n",
    "        mfccs.append([extracted_features, df['label'][j]])\n",
    "    df2 = pd.DataFrame(mfccs, columns=['data','label'])\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcdfde28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 124)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['data'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "469dc24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        df = get_mfcc(make_base_dataset())\n",
    "        self.x = get_mfcc(df['data'])\n",
    "        self.y = df['label']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x[idx])\n",
    "        y = self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b392f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
