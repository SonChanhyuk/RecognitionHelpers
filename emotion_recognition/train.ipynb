{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "657a7770",
   "metadata": {},
   "source": [
    "# Speech Emotion Recognition \n",
    "\n",
    "emotions: [happy, sad, neutral, fear, angry, disgust]\n",
    "\n",
    "Datasets:\n",
    "* Crowd-sourced Emotional Multimodal Actors Dataset (Crema-D)\n",
    "* Ryerson Audio-Visual Database of Emotional Speech and Song (Ravdess)\n",
    "* Surrey Audio-Visual Expressed Emotion (Savee)\n",
    "* Toronto Emotional Speech Set (Tess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a178983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "from torchvision.models import googlenet, resnet18, resnet50\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import functional as TF\n",
    "import sklearn\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ab88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/ejlok1/cremad\")\n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio\")\n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/ejlok1/surrey-audiovisual-expressed-emotion-savee\")\n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f139baa0",
   "metadata": {},
   "source": [
    "## fix random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b28ab408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = 20236\n",
    "deterministic = True\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "if deterministic:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7359497d",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6d3097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAVDESS = \"test/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n",
    "CREMA = \"test/cremad/AudioWAV/\"\n",
    "TESS = \"test/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n",
    "SAVEE = \"test/surrey-audiovisual-expressed-emotion-savee/ALL/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9109cc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/ravdess-emotional-speech-audio/audio_spee...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/ravdess-emotional-speech-audio/audio_spee...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/ravdess-emotional-speech-audio/audio_spee...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/ravdess-emotional-speech-audio/audio_spee...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/ravdess-emotional-speech-audio/audio_spee...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  emotion\n",
       "0  test/ravdess-emotional-speech-audio/audio_spee...  neutral\n",
       "1  test/ravdess-emotional-speech-audio/audio_spee...  neutral\n",
       "2  test/ravdess-emotional-speech-audio/audio_spee...  neutral\n",
       "3  test/ravdess-emotional-speech-audio/audio_spee...  neutral\n",
       "4  test/ravdess-emotional-speech-audio/audio_spee...    happy"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ravdess_dir_lis = os.listdir(RAVDESS)\n",
    "path_list = []\n",
    "gender_list = []\n",
    "emotion_list = []\n",
    "\n",
    "emotion_dic = {\n",
    "    '03' : 'happy',\n",
    "    '01' : 'neutral',\n",
    "    '04' : 'sad',\n",
    "    '05' : 'angry',\n",
    "    '06' : 'fear',\n",
    "    '07' : 'disgust',\n",
    "}\n",
    "\n",
    "for directory in ravdess_dir_lis:\n",
    "    actor_files = os.listdir(os.path.join(RAVDESS, directory))\n",
    "    for audio_file in actor_files: \n",
    "        part = audio_file.split('.')[0]\n",
    "        key = part.split('-')[2]\n",
    "        if key in emotion_dic:\n",
    "            gender_code = int(part.split('-')[6])\n",
    "            path_list.append(f\"{RAVDESS}{directory}/{audio_file}\")\n",
    "            gender_list.append('female' if gender_code & 1 == 0 else 'male')\n",
    "            emotion_list.append(emotion_dic[key])\n",
    "            \n",
    "ravdess_df = pd.concat([\n",
    "    pd.DataFrame(path_list, columns=['path']),\n",
    "    pd.DataFrame(emotion_list, columns=['emotion'])\n",
    "], axis=1)\n",
    "\n",
    "ravdess_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a303f6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/cremad/AudioWAV/1001_IEO_ANG_HI.wav</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/cremad/AudioWAV/1001_IEO_DIS_HI.wav</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/cremad/AudioWAV/1001_IEO_FEA_HI.wav</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/cremad/AudioWAV/1001_IEO_HAP_HI.wav</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/cremad/AudioWAV/1001_IEO_SAD_HI.wav</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path  emotion\n",
       "0  test/cremad/AudioWAV/1001_IEO_ANG_HI.wav    angry\n",
       "1  test/cremad/AudioWAV/1001_IEO_DIS_HI.wav  disgust\n",
       "2  test/cremad/AudioWAV/1001_IEO_FEA_HI.wav     fear\n",
       "3  test/cremad/AudioWAV/1001_IEO_HAP_HI.wav    happy\n",
       "4  test/cremad/AudioWAV/1001_IEO_SAD_HI.wav      sad"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crema_dir_list = os.listdir(CREMA)\n",
    "path_list = []\n",
    "gender_list = []\n",
    "emotion_list = []\n",
    "\n",
    "emotion_dic = {\n",
    "    'HAP' : 'happy',\n",
    "    'NEU' : 'neutral',\n",
    "    'SAD' : 'sad',\n",
    "    'ANG' : 'angry',\n",
    "    'FEA' : 'fear',\n",
    "    'DIS' : 'disgust',\n",
    "}\n",
    "\n",
    "female_id_list = [\n",
    "    '1002', '1003', '1004', '1006', '1007', '1008', '1009', '1010', '1012', '1013', '1018', \n",
    "    '1020', '1021', '1024', '1025', '1028', '1029', '1030', '1037', '1043', '1046', '1047', \n",
    "    '1049', '1052', '1053', '1054', '1055', '1056', '1058', '1060', '1061', '1063', '1072', \n",
    "    '1073', '1074', '1075', '1076', '1078', '1079', '1082', '1084', '1089', '1091',\n",
    "]\n",
    "\n",
    "for audio_file in crema_dir_list:\n",
    "    part = audio_file.split('_')\n",
    "    key = part[2]\n",
    "    if key in emotion_dic and part[3] == 'HI.wav':\n",
    "        path_list.append(f\"{CREMA}{audio_file}\")\n",
    "        gender_list.append('female' if part[0] in female_id_list else 'male')\n",
    "        emotion_list.append(emotion_dic[key])\n",
    "\n",
    "crema_df = pd.concat([\n",
    "    pd.DataFrame(path_list, columns=['path']),\n",
    "    pd.DataFrame(emotion_list, columns=['emotion'])\n",
    "], axis=1)\n",
    "\n",
    "crema_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73842cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/toronto-emotional-speech-set-tess/tess to...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/toronto-emotional-speech-set-tess/tess to...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/toronto-emotional-speech-set-tess/tess to...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/toronto-emotional-speech-set-tess/tess to...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/toronto-emotional-speech-set-tess/tess to...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path emotion\n",
       "0  test/toronto-emotional-speech-set-tess/tess to...   angry\n",
       "1  test/toronto-emotional-speech-set-tess/tess to...   angry\n",
       "2  test/toronto-emotional-speech-set-tess/tess to...   angry\n",
       "3  test/toronto-emotional-speech-set-tess/tess to...   angry\n",
       "4  test/toronto-emotional-speech-set-tess/tess to...   angry"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tess_dir_list = os.listdir(TESS)\n",
    "path_list = []\n",
    "gender_list = []\n",
    "emotion_list = [] \n",
    "\n",
    "emotion_dic = {\n",
    "    'happy'   : 'happy',\n",
    "    'neutral' : 'neutral',\n",
    "    'sad'     : 'sad',\n",
    "    'Sad'     : 'sad',\n",
    "    'angry'   : 'angry',\n",
    "    'fear'    : 'fear',\n",
    "    'disgust'  : 'disgust',\n",
    "}\n",
    "\n",
    "for directory in tess_dir_list:\n",
    "    audio_files = os.listdir(os.path.join(TESS, directory))\n",
    "    for audio_file in audio_files:\n",
    "        part = audio_file.split('.')[0]\n",
    "        key = part.split('_')[2]\n",
    "        if key in emotion_dic:\n",
    "            path_list.append(f\"{TESS}{directory}/{audio_file}\") \n",
    "            gender_list.append('female') # female only dataset\n",
    "            emotion_list.append(emotion_dic[key])\n",
    "            \n",
    "tess_df = pd.concat([\n",
    "    pd.DataFrame(path_list, columns=['path']),\n",
    "    pd.DataFrame(emotion_list, columns=['emotion'])\n",
    "], axis=1)\n",
    "\n",
    "tess_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fb14278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/surrey-audiovisual-expressed-emotion-save...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/surrey-audiovisual-expressed-emotion-save...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/surrey-audiovisual-expressed-emotion-save...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/surrey-audiovisual-expressed-emotion-save...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/surrey-audiovisual-expressed-emotion-save...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path emotion\n",
       "0  test/surrey-audiovisual-expressed-emotion-save...   angry\n",
       "1  test/surrey-audiovisual-expressed-emotion-save...   angry\n",
       "2  test/surrey-audiovisual-expressed-emotion-save...   angry\n",
       "3  test/surrey-audiovisual-expressed-emotion-save...   angry\n",
       "4  test/surrey-audiovisual-expressed-emotion-save...   angry"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savee_dir_list = os.listdir(SAVEE)\n",
    "path_list = []\n",
    "gender_list = []\n",
    "emotion_list = []\n",
    "\n",
    "emotion_dic = {\n",
    "    'h'  : 'happy',\n",
    "    'n'  : 'neutral',\n",
    "    'sa' : 'sad',\n",
    "    'a'  : 'angry',\n",
    "    'f'  : 'fear',\n",
    "    'd'  : 'disgust'\n",
    "}\n",
    "\n",
    "for audio_file in savee_dir_list:\n",
    "    part = audio_file.split('_')[1]\n",
    "    key = part[:-6]\n",
    "    if key in emotion_dic:\n",
    "        path_list.append(f\"{SAVEE}{audio_file}\")\n",
    "        gender_list.append('male') # male only dataset\n",
    "        emotion_list.append(emotion_dic[key])\n",
    "        \n",
    "savee_df = pd.concat([\n",
    "    pd.DataFrame(path_list, columns=['path']),\n",
    "    pd.DataFrame(emotion_list, columns=['emotion'])\n",
    "], axis=1)\n",
    "\n",
    "savee_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa3b78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/ravdess-emotional-speech-audio/audio_spee...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/ravdess-emotional-speech-audio/audio_spee...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/ravdess-emotional-speech-audio/audio_spee...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/ravdess-emotional-speech-audio/audio_spee...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/ravdess-emotional-speech-audio/audio_spee...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  emotion\n",
       "0  test/ravdess-emotional-speech-audio/audio_spee...  neutral\n",
       "1  test/ravdess-emotional-speech-audio/audio_spee...  neutral\n",
       "2  test/ravdess-emotional-speech-audio/audio_spee...  neutral\n",
       "3  test/ravdess-emotional-speech-audio/audio_spee...  neutral\n",
       "4  test/ravdess-emotional-speech-audio/audio_spee...    happy"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([\n",
    "    ravdess_df, \n",
    "    crema_df, \n",
    "    tess_df, \n",
    "    savee_df\n",
    "], axis=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7caa6582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from pydub import AudioSegment, effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fd9b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(path):\n",
    "    _, sr = librosa.load(path)\n",
    "    raw_audio = AudioSegment.from_file(path)\n",
    "    \n",
    "    samples = np.array(raw_audio.get_array_of_samples(), dtype='float32')\n",
    "    trimmed, _ = librosa.effects.trim(samples, top_db=25)\n",
    "    if (len(trimmed)>180000):\n",
    "        padded = trimmed[:180000]\n",
    "    else:\n",
    "        padded = np.pad(trimmed, (0, 180000-len(trimmed)), 'constant')\n",
    "    return padded, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "453cddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dic = {\n",
    "    'neutral' : 0,\n",
    "    'happy'   : 1,\n",
    "    'sad'     : 2, \n",
    "    'angry'   : 3, \n",
    "    'fear'    : 4, \n",
    "    'disgust' : 5\n",
    "}\n",
    "\n",
    "def encode(label):\n",
    "    return emotion_dic.get(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09857ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "zcr_list = []\n",
    "rms_list = []\n",
    "mfccs_list = []\n",
    "emotion_list = []\n",
    "\n",
    "FRAME_LENGTH = 2048\n",
    "HOP_LENGTH = 512\n",
    "\n",
    "for row in df.itertuples(index=False):\n",
    "    try: \n",
    "        y, sr = preprocess_audio(row.path)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n",
    "        rms = librosa.feature.rms(y=y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=HOP_LENGTH)\n",
    "        zcr_list.append(zcr)\n",
    "        rms_list.append(rms)\n",
    "        mfccs_list.append(mfccs)\n",
    "\n",
    "        emotion_list.append(encode(row.emotion))\n",
    "    except:\n",
    "        print(f\"Failed for path: {row.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "209bfbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((\n",
    "    np.swapaxes(zcr_list, 1, 2), \n",
    "    np.swapaxes(rms_list, 1, 2), \n",
    "    np.swapaxes(mfccs_list, 1, 2)), \n",
    "    axis=2\n",
    ")\n",
    "X = X.astype('float32')\n",
    "\n",
    "y = np.asarray(emotion_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbc63ae",
   "metadata": {},
   "source": [
    "## LSTM train code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "215a6752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.emotions = torch.Tensor(labels).to(torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emotions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.inputs[idx]\n",
    "        labels = self.emotions[idx]\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65312b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionLSTM(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_size, n_class):\n",
    "        super(EmotionLSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=input_shape[1], hidden_size=hidden_size, batch_first=True, dropout = 0.3)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True, dropout=0.3)\n",
    "        self.lstm3 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size//2, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_size//2, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = x[:, -1, :] \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2283a0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmotionLSTM(X.shape[1:3],512,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b42e19",
   "metadata": {},
   "source": [
    "## resnet train code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71217841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = []\n",
    "        self.emotions = torch.Tensor(labels).to(torch.int64)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        for i in tqdm(range(len(images))):\n",
    "            \n",
    "            # MFCC를 이미지로 변환하여 크기를 조정합니다.\n",
    "            image = Image.fromarray(images[i],\"RGB\")\n",
    "            image = transform(image)\n",
    "            self.images.append(image)\n",
    "            \n",
    "        self.len = len(images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.emotions[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b2d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "num_classes = 6\n",
    "model.fc = nn.Linear(num_features, num_classes) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb60a923",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b8ab455",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EmotionDataset(X,y)\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "validation_size = int(dataset_size * 0.1)\n",
    "test_size = dataset_size - train_size - validation_size\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34c59a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=4, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa2204af",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e601ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a7d574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cf4d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54859430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/200]: 100%|██████████████████████████████████████████████████████| 216/216 [00:16<00:00, 13.21it/s, loss=1.57]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:03<00:00, 33.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 32.407407407407405%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/200]: 100%|██████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.63it/s, loss=1.45]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:03<00:00, 35.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 33.7962962962963%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/200]: 100%|██████████████████████████████████████████████████████| 216/216 [00:15<00:00, 13.64it/s, loss=1.49]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:03<00:00, 35.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 40.0462962962963%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/200]: 100%|███████████████████████████████████████████████████████| 216/216 [00:16<00:00, 13.49it/s, loss=1.4]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:03<00:00, 35.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 37.26851851851852%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/200]: 100%|██████████████████████████████████████████████████████| 216/216 [00:16<00:00, 13.18it/s, loss=1.39]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:02<00:00, 36.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 41.43518518518518%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/200]: 100%|██████████████████████████████████████████████████████| 216/216 [00:15<00:00, 13.78it/s, loss=1.35]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:03<00:00, 35.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 39.583333333333336%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/200]: 100%|██████████████████████████████████████████████████████| 216/216 [00:16<00:00, 12.77it/s, loss=1.32]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:02<00:00, 36.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 45.833333333333336%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/200]: 100%|██████████████████████████████████████████████████████| 216/216 [00:16<00:00, 13.12it/s, loss=1.23]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:03<00:00, 34.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 41.43518518518518%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/200]: 100%|███████████████████████████████████████████████████████| 216/216 [00:16<00:00, 13.13it/s, loss=1.2]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:03<00:00, 35.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 43.98148148148148%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/200]: 100%|█████████████████████████████████████████████████████| 216/216 [00:15<00:00, 14.01it/s, loss=1.21]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:02<00:00, 36.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 45.370370370370374%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/200]: 100%|█████████████████████████████████████████████████████| 216/216 [00:15<00:00, 14.16it/s, loss=1.16]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:02<00:00, 36.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 48.611111111111114%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/200]: 100%|█████████████████████████████████████████████████████| 216/216 [00:16<00:00, 13.28it/s, loss=1.16]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:02<00:00, 36.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 45.601851851851855%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [13/200]: 100%|█████████████████████████████████████████████████████| 216/216 [00:15<00:00, 14.34it/s, loss=1.15]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:02<00:00, 36.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 47.4537037037037%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [14/200]: 100%|██████████████████████████████████████████████████████| 216/216 [00:15<00:00, 13.91it/s, loss=1.2]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:02<00:00, 37.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 44.675925925925924%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [15/200]: 100%|█████████████████████████████████████████████████████| 216/216 [00:14<00:00, 14.58it/s, loss=1.19]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:03<00:00, 35.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 50.925925925925924%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [16/200]: 100%|██████████████████████████████████████████████████████| 216/216 [00:15<00:00, 13.71it/s, loss=1.3]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:02<00:00, 37.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 47.916666666666664%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [17/200]: 100%|█████████████████████████████████████████████████████| 216/216 [00:14<00:00, 14.67it/s, loss=1.16]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:02<00:00, 37.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 53.47222222222222%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [18/200]: 100%|█████████████████████████████████████████████████████| 216/216 [00:15<00:00, 14.13it/s, loss=1.18]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:02<00:00, 36.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 47.916666666666664%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [19/200]: 100%|█████████████████████████████████████████████████████| 216/216 [00:16<00:00, 13.21it/s, loss=1.11]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:02<00:00, 36.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 56.48148148148148%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [20/200]:  87%|█████████████████████████████████████████████▎      | 188/216 [00:14<00:02, 12.83it/s, loss=0.926]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     20\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accuracy_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    loop = tqdm(dataloader, total=len(dataloader), leave=True)\n",
    "    model.train()\n",
    "    \n",
    "    for images, labels in loop:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        loop.set_description(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        loop.set_postfix(loss=running_loss / (len(dataloader)))\n",
    "    accuracy_list.append(test(model,test_loader))\n",
    "    torch.save(model.state_dict(), \"result/model_LSTM_512_{}.pth\".format(epoch))\n",
    "    \n",
    "print('Training finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb15361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
